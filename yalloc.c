/* yalloc.c - yet another memory allocator

   This file is part of yalloc, yet another memory allocator providing affordable safety in a compact package.

   SPDX-FileCopyrightText: Â© 2024 Joris van der Geer
   SPDX-License-Identifier: GPL-3.0-or-later

  A 'heap' is the toplevel structure to hold all admin.
  Memory ranges are obtained from the OS as large power-of-two sized regions. Each region has separately mmap()ed user data and metadata.
  User blocks above a given size are mmap()ed directly, described by a virtual region.
  Initial regions are of a given size, subsequent regions may be larger dependent on overall usage.

  Regions are described by a region descriptor table, similar to how multi-level page tables describe virtual memory.
  A single top-level directory holds entries to mid-level tables.
  These in turn hold entries to leaf tables. The latter holds region pointers per OS memory page.
  free() and realloc() uses these to locate an entry, given the minimum region size. Valid pointers are guaranteed by leading to a region and being at a valid cell start.

  Within a region, user data is kept separate from admin aka metadata. This protects metadata from being overwriitten
  The user data is a single block, consisting of fixed-size cells. The metadata contains entries per cell.
  User blocks have no header or trailer. Consecutively allocated blocks are adjacent without a gap. This helps cache and TLB efficiency.
  Once a region becomes fully free, it is returned to the os.

  Blocks are aligned following 'weak alignment' as in https://www.open-std.org/JTC1/SC22/WG14/www/docs/n2293.htm
  Thus, small blocks follow the alignment of the largest type that fits in. 2=2 3=4 4=4 5=8 ...

  Freed blocks are held in a recycling bin aka freelist, genuinely freeing a LRU item. malloc() uses these on an MRU basis if available.
  In additon, each cell has a 'free' marker used to detect double free.

  Multiple threads are supported by having each thread contain a full heap. Allocations are always local in a thread's own heap.
  If free / realloc cannot locate a block [in the local heap], a global region descriptor tabel is consulted. This table holds a cumulative region table and is updated atomically.
  Each region contains a local and remote freelist.
  A free or realloc from the same thread is taken from the local freelist without atomics (except double-free detect) or locking.
  Free or realloc from a different thread is handled by adding it to the owner region's remote freelist.
  This is handled by a set of atomic compare-swap and a tiny, nonblocking interlock. A subsequent alloc request will inspect the local freelist first.
  If empty, the remote freelist is checked and a nonblocking 'trylock' is used to remove the entry.
  For realloc(), the size can be obtained first. If a change is needed, a new block is allocated from the local heap, and the free of the original block is handled
  as with a free().

  Double-free detection is done using atomic compare-swap, to detect double or concurrent free / realloc in the presence of multiple threads.
  This is independent from the freelist binning described above, yet must be done before adding a block to a bin, as otherwise a subsequent alloc
  would hand out the same block twice.

  Regions are created on-demand when no existing region has space.  After a certain number of free() calls, a trimming scan runs to check for empty regions.
  These are first cleaned back to initial. In a next round and stull unused, they are moved to a region recycling state.
  New regions can reuse these. If not reused after a certain number of scans, the underying user memory is freed.
  */

static const char yal_version[] = "0.8.0-alpha.0";

// Note: after headers with declarations and macros only, all modules with function bodies are included as .h

#include "config_gen.h" // generated by configure.c

#ifndef Config_gen // fallback defaults
  #define Sys_page 12
  typedef unsigned long ub8;
#endif

#include "config.h" // user config, first header

#if Yal_signal
 #define _POSIX_C_SOURCE 199309L // needs to be at first system header
 #include <signal.h>
#endif

#include <stddef.h> // size_t
#include <limits.h> // UINT_MAX

#include <stdarg.h> // va_list (diag.h)
#include <string.h> // memset memcpy

extern char *getenv(const char *name); // no <stdlib.h> as its malloc() defines may not be compatible

#ifdef Yal_stats_envvar
  extern int atexit(void (*function)(void));
#endif
#include "stdlib.h" // our own

#if Yal_errno
 #include <errno.h>
 #define Enomem errno = ENOMEM;
 #define Einval errno = EINVAL;
#else
 #define Enomem // empty
 #define Einval
#endif

/* Support using Valgrind without replacing malloc as if replaced.
 * using vg client requests to emulate memcheck's checks
 * These calls add minimal overhead when not running in vg
 * typical usage: valgrind --tool=memcheck--soname-synonyms=somalloc=nouserintercept
 */
#if Yal_enable_valgrind
 #include <valgrind/valgrind.h>
 #include <valgrind/memcheck.h>
 #include <valgrind/drd.h>
 #define vg_mem_noaccess(p,n) VALGRIND_MAKE_MEM_NOACCESS((p),(n));
 #define vg_mem_undef(p,n) VALGRIND_MAKE_MEM_UNDEFINED((p),(n));
 #define vg_mem_def(p,n) VALGRIND_MAKE_MEM_DEFINED((p),(n));

 #define vg_atom_before(adr) ANNOTATE_HAPPENS_BEFORE((adr));
 #define vg_atom_after(adr) ANNOTATE_HAPPENS_AFTER((adr));

 #define vg_drd_rwlock_init(p) ANNOTATE_RWLOCK_CREATE((p));
 #define vg_drd_wlock_acq(p) ANNOTATE_WRITERLOCK_ACQUIRED((p));
 #define vg_drd_wlock_rel(p) ANNOTATE_WRITERLOCK_RELEASED((p));

static int vg_mem_isaccess(void *p,size_t n) // accessible when not expected
{
  if (RUNNING_ON_VALGRIND == 0) return 0;

  return VALGRIND_CHECK_MEM_IS_ADDRESSABLE(p,n) == 0;
}

#else
 #define vg_mem_noaccess(p,n)
 #define vg_mem_undef(p,n)
 #define vg_mem_def(p,n)
 #define vg_mem_isaccess(p,n) 0

 #define vg_atom_before(adr)
 #define vg_atom_after(adr)

 #define vg_drd_rwlock_init(p)
 #define vg_drd_wlock_acq(p)
 #define vg_drd_wlock_rel(p)

#endif

#include "base.h"

#include "malloc.h" // nonstandard, common extensions

typedef struct yal_stats yalstats;

extern Noret void _Exit(int status);

#include "yalloc.h"

#include "util.h"

#include "atom.h"

// -- start derived config --

#define Dir1len (1u << Dir1)
#define Dir2len (1u << Dir2)
#define Dir3len (1u << Dir3)

#define Maxclass (Mmap_max_threshold + 3)
#define Clascnt (Maxclass * 4)
#define Xclascnt (32 * 4)

#define Regorder 36

static const unsigned long  mmap_max_limit = (1ul << min(Mmap_max_threshold,Hi30));
static const unsigned long  mmap_limit = (1ul << Mmap_threshold);

#if defined Page_override && Page_override > 4 // from config.h
 #define Page Page_override
#else
 #define Page Sys_page // as determined by ./configure
#endif

#if Page >= 32
 #error "Page needs to be power of two of Pagesize"
#endif
#define Pagesize (1u << Page)

#define Stdalign1 (Stdalign - 1)

#if Pagesize > 65536
  #undef Minilen
  #undef Bumplen
  #define Minilen 0
  #define Bumplen 0
#endif

#define Clasregs 32 // ~Vmbits - Maxregion, ub4 clasmsk

#if Yal_enable_tag
  #define Tagarg(t) ,t
#else
  #define Tagarg(t)
#endif

// -- end derived config --

#ifdef Inc_os
  #include "os.c"
#else
  #include "os.h"
#endif

#include "printf.h"

// -- diagnostics --

enum File { Falloc,Fatom,Fbist,Fboot,Fbump,Fdbg,Fdiag,Ffree,Fheap,Fmini,Frealloc,Fregion,Fslab,Fstat,Fstd,Fyalloc,Fcount };
static cchar * const filenames[Fcount] = {
  "alloc.h","atom","bist.h","boot.h","bump.h","dbg.h","diag.h","free.h","heap.h","mini.h","realloc.h","region.h","slab.h","stats.h","std.h","yalloc.c"
};

enum Loglvl { Fatal,Assert,Error,Warn,Info,Trace,Vrb,Debug,Nolvl };
static cchar * const lvlnames[Nolvl + 1] = { "Fatal","Assert","Error","Warn","Info","Trace","Vrb","Debug"," " };

enum Loc { Lnone,
  Lreal = 1,Lfree = 2,Lsize = 3,Lalloc = 4,Lallocal = 5,Lcalloc = 6,Lstats = 7,Ltest = 8,Lsig = 9,
  Lmask = 15,Lremote = 16,
  Lrreal = 1 + 16, Lrfree = 2 + 16,
  Lrsize = 3 + 16 };
static cchar * const locnames[Lmask + 1] = { " ","realloc","free","size","malloc","allocal","calloc","stats","test","signal","?","?","?","?","?","?" };

static _Atomic ub4 g_errcnt;
static _Atomic ub4 g_msgcnt;

static _Atomic unsigned long mypid;

static char global_cmdline[256];

static Cold ub4 diagfln(char *buf,ub4 pos,ub4 len,ub4 fln)
{
  char fbuf[64];
  ub4 fn = fln >> 16;
  ub4 ln = fln & Hi16;
  cchar *fnam;

  if (fn < Fcount) {
    fnam = filenames[fn];
    snprintf_mini(fbuf,0,64,"yal/%s:%-3u",fnam,ln);
  } else snprintf_mini(fbuf,0,64,"yal/(%u):%-3u",fn,ln);
  pos += snprintf_mini(buf,pos,len,"%17s ",fbuf);
  return pos;
}

// simple diag, see diag.h for elaborate
static Printf(5,6) ub4 minidiag(ub4 fln,enum Loc loc,enum Loglvl lvl,ub4 id,char *fmt,...)
{
  va_list ap;
  char buf[256];
  ub4 pos = 0,len = 254;
  cchar *lvlnam = lvl < Nolvl ? lvlnames[lvl] : "?";
  cchar *locnam = locnames[loc & Lmask];
  int fd,fd2;

  if (lvl > Yal_log_level) return 0;

  Atomad(g_msgcnt,1,Moacqrel);

  if (*fmt == '\n')  buf[pos++] = *fmt++;
  pos = diagfln(buf,pos,len,fln);

  pos += snprintf_mini(buf,pos,len,"%-5lu %-4u %-3u %c %-8s ",Atomget(mypid,Monone),id,0,*lvlnam,locnam);

  va_start(ap,fmt);
  pos += mini_vsnprintf(buf,pos,len,fmt,ap);
  va_end(ap);

  if (pos < 255) buf[pos++] = '\n';

  if (lvl > Error) {
    fd = fd2 = Yal_log_fd;
    if (fd == -1) return pos;
  } else {
    fd = Yal_err_fd;
    if (fd == -1) fd = 2;
    fd2 = Yal_Err_fd;
  }
  oswrite(fd,buf,pos,__LINE__);
  if (fd2 != fd) oswrite(fd2,buf,pos,__LINE__);
  if (loc == Lsig) {
    return pos;
  }
  if (lvl < Warn) _Exit(1);
  return pos;
}

// -- main admin structures --

enum Rtype { Rnone,Rslab,Rbump,Rmini,Rmmap,Rcount };
static cchar * const regnames[Rcount + 1] = { "none","slab","bump","mini","mmap", "?" };

enum Status { St_ok, St_oom,St_tmo,St_intr,St_error,St_free2,St_nolock,St_trim };

typedef unsigned char celset_t;

struct overflow {
  struct st_region *reg; // implied heap at reg.bas.hid
  ub4 cel;
  ub4 filler;
};

// per-region statistics
struct regstat {
  size_t allocs,Allocs,callocs,reallocles,reallocgts,binallocs,iniallocs,xallocs;
  size_t frees,rfrees;
  size_t rbinskip;
  ub4 minlen,maxlen;
  size_t rbin;
  size_t invalidfrees;
  ub4 aligns[32];
};

struct Align(16) st_xregion { // base, only type

  // + common
  size_t user; // user aka client block
  size_t len; // gross client block len as allocated

  struct st_heap *hb;

  _Atomic ub4 lock;
  enum Rtype typ;

  ub4 hid;
  ub4 id;

  ub4 age;
  ub4 aged;

  // - common

  ub4 filler;
};
typedef struct st_xregion xregion;

struct Align(16) st_mpregion { // mmap region. allocated as pool from heap

  // + common
  size_t user; // user aka client block
  size_t len; // gross client block len as allocated

  struct st_heap *hb;

  _Atomic ub4 lock;
  enum Rtype typ;

  ub4 hid;
  ub4 id;

  ub4 age;
  ub4 aged;

  // - common

  _Atomic ub4 set; // 0 never used 1 alloc 2 free

  size_t ulen;  // gross minus user aka net len

  size_t align;  // offset if alioc_align > pagesize
  ub4 order;
  ub4 filler;

  struct st_mpregion *nxt; // for aging and stats
  struct st_mpregion *frenxt; // for reuse aka regbin
  ub8 filler8;
};
typedef struct st_mpregion mpregion;

struct Align(16) st_bregion { // bump region. statically present in heap

  // + common
  size_t user; // user aka client block
  size_t len; // gross client block len as allocated

  struct st_heap *hb;

  _Atomic ub4 lock;
  enum Rtype typ;

  ub4 hid;
  ub4 id;

  ub4 age;
  ub4 aged;

  // - common

  ub4 metalen;  // metadata aka admin size
  ub4 *meta;  // fmetadata aka admin

  // in ub4
  ub4 freorg;
  ub4 tagorg;

  ub8 uid;

  ub4 pos;
  ub4 cnt;

  // stats
  ub4 allocs;
  _Atomic ub4 frees;
  ub4 albytes,frebytes;
};
typedef struct st_bregion bregion;

struct Align(16) st_region { // slab region. allocated as pool from heap

  // + common
  size_t user; // user aka client block
  size_t len; // client block len

  struct st_heap *hb;

  _Atomic ub4 lock;
  enum Rtype typ;

  ub4 hid;
  ub4 id;

  ub4 age;
  ub4 aged;

  // - common

  ub4 cellen; // gross and aligned cel len

  ub4 * meta;  // metadata aka admin
  size_t metalen;  // metadata aka admin size

  ub4 inipos; // never-allocated marker
  ub4 clas;

  ub8 uid; // unique for each creation or reuse

  struct st_region *nxt; // for aging and stats
  struct st_region *frenxt; // for reuse aka regbin

  ub4 celcnt;

  ub4 claspos;
  ub4 clr; // set if calloc needs to clear

  // bin
  ub4 binpos;

  ub4 bucket;

  ub4 claseq;
  ub4 celord;  //   cel len if pwr2 0 if not
  ub4 cntord;
  ub4 order; // region len = 1 << order

  size_t binorg; // offset in meta
  size_t lenorg;
  size_t tagorg;

  // remote bin
  ub4 * _Atomic rembin; // allocated on demand by sender from sender's heapmem

  _Atomic ub4 refcnt; // todo ?
  ub4 rbinpos;

  struct regstat stat;

  size_t metautop; // as required
  ub8 filler;
};
typedef struct st_region region;

// thread heap base including starter kit. page-aligned
struct Align(16) st_heap {
  _Atomic ub4 lock;
  ub4 id; // ident

  char l1fill[L1line - 8];

  struct st_xregion *mrufrereg; // todo check reuse

  // slab allocator
  ub4 clascnts[Xclascnt]; // track popularity of sizes
  ub4 claslens[Xclascnt]; // size covered

  ub2 claspos[Clascnt]; // currently used
  ub4 clasmsk[Clascnt]; // bit mask for clasregs having space

  ub2 clasregcnt[Clascnt]; // #regions per class

  struct st_region *clasregs[Clascnt * Clasregs];

  // region bases
  struct st_region *regmem;
  struct st_mpregion *xregmem;
  ub4 regmem_pos;
  ub4 xregmem_pos;

  // mrf list of freed regions, per order
  struct st_region * freeregs[Regorder + 1];
  struct st_mpregion* freempregs[Vmbits - Mmap_threshold + 1];

  struct st_heap *nxt; // list for reassign

  // page dir root
  struct st_xregion *** rootdir[Dir1len];

  // starter mem for dir pages
  struct st_xregion ***dirmem;
  struct st_xregion **leafdirmem;
  ub4 dirmem_pos,ldirmem_pos;
  ub4 dirmem_top,ldirmem_top;

  // region lists
  struct st_region *reglst,*regprv,*regtrim;// todo prv for stats ?
  struct st_mpregion *mpreglst,*mpregprv,*mpregtrim;

  // mempool for rembins
  ub4 *rbinmem;
  ub4 rbmempos,rbmemlen;

  ub4 trimcnt;
  ub4 filler4;

  // bump allocator
  struct st_bregion bumpregs[Bumpregions];

  _Atomic ub4 ticker;
  _Atomic ub4 curtic;

  struct yal_stats stat;
  ub8 filler;
};
typedef struct st_heap heap;

// local buffering for remote free
struct remote {
  ub4 remcels[Rembkt * Rembatch]; // list of cels per bucket
  ub2 remcnts[Rembkt]; // bucket size
  struct st_region *remregs[Rembkt];
  ub8 remuids[Rembkt];

  struct overflow ovfmem[Ovflen]; // starter kit
  struct overflow xovfmem[Ovfxlen]; // starter kit
  struct overflow *ovfbin;
  struct overflow *xovfbin;
  Ub8 bktmsk,bkt1msk;
  ub4 ovfpos;
  ub4 ovflen,xovflen;
  ub4 filler;
};

struct hdstats {
  ub4 newheaps,useheaps;
  size_t getheaps,nogetheaps,nogetheap0s;
  ub4 xmaxbin,xmaxovf;
  ub4 maxspin;
  ub4 nolink;
  size_t xfreebuf,xfreebatch;
  size_t alloc0s,free0s,freenils;
  size_t xslabfrees;
  size_t xminifrees;
  size_t invalid_frees,errors;
  size_t xmapfrees;
  size_t xbumpfrees,xbumpfreebytes;
  size_t xfreesum,xfreebin;
  size_t remote_dropped,remote_dropbytes;
  size_t delregions,munmaps;
};

#if Yal_thread_exit // install thread exit handler to recycle heap descriptor
  #include <pthread.h>
  #include "thread.h"

#else
  #define Thread_clean_info int
#endif

#define Miniord 16

struct st_heapdesc {
  struct st_heapdesc *nxt,*frenxt;
  struct st_heap *hb;
  struct st_bregion *mhb;
  struct remote *rem;

  char *errbuf;

  ub4 errfln;
  ub4 id;

  enum Status status;
  _Atomic ub4 lock;
  ub4 locked;

  size_t getheaps,nogetheaps;

  struct hdstats stat;

  ub1 minicnts[Miniord - 4];
  ub4 minidir;

#if Yal_enable_stack
  ub4 flnstack[8];
  ub4 flnpos;
//  ub4 tag;
#endif

  Thread_clean_info thread_clean_info;
};
typedef struct st_heapdesc heapdesc;

static ub4 global_stats_opt; // set from Yal_stats_envvar
static ub4 global_trace = Yal_trace_default;
static _Atomic ub4 global_check = Yal_check_default;

static heapdesc * _Atomic global_freehds;

#if Yal_thread_exit // install thread exit handler to recycle heap descriptor
  // add hd to head of free list
  static void thread_cleaner(void *arg)
  {
     heapdesc *hd = (heapdesc *)arg;
     heapdesc *prv = Atomget(global_freehds,Moacq);

     // minidiag( (Fyalloc << 16) | __LINE__,Lnone,Info,0,"thread %u exit ",hd->id);

     hd->frenxt = prv;
     Cas(global_freehds,prv,hd);
  }

  static void thread_setclean(heapdesc *hd)
  {
     Thread_clean_push(&hd->thread_clean_info,thread_cleaner, hd);
  }
#else
  static void thread_setclean(heapdesc *hd) { hd->thread_clean_info = 0; }
#endif

static size_t zeroarea[8];
static size_t *zeroblock = zeroarea + 4; // malloc(0)

#include "boot.h"

// -- global heap structures and access

static _Thread_local struct st_heapdesc *thread_heap;

static struct st_heapdesc * _Atomic global_heapdescs;
static struct st_heap * _Atomic global_heaps;

static _Atomic ub4 global_tid = 1;
static _Atomic ub4 global_hid = 1;

static Hot heapdesc *getheapdesc(enum Loc loc)
{
  heapdesc *org,*hd = thread_heap;
  ub4 id,iter;
  ub4 fln;
  ub4 len = sizeof(struct st_heapdesc);
  bool didcas;

  if (likely(hd != nil)) return hd;

  id = Atomad(global_tid,1,Moacqrel);

  fln = Fyalloc << 16;
  if (id < 3) minidiag(fln|__LINE__,loc,Info,id,"new base heap size %u.%u",len,(ub4)sizeof(struct hdstats));
  len = doalign4(len,L1line);

  // reuse ?
  hd = Atomget(global_freehds,Moacq);
  if (hd) {
    org = hd->frenxt;
    didcas = Cas(global_freehds,hd,org);
    if (didcas) {
      thread_heap = hd;
      thread_setclean(hd);
      hd->hb = nil;
      return hd;
    }
  }

  // new
  hd = bootalloc(fln|__LINE__,id,Lnone,len);
  if (unlikely(hd == nil)) {
    minidiag(fln|__LINE__,loc,Fatal,id,"cannot allocate heap descriptor %u",id);
    _Exit(1);
  }

  thread_heap = hd;
  thread_setclean(hd);

  iter = 10;

  // create list of all heapdescs for stats
  do {
    org = hd->nxt = Atomget(global_heapdescs,Moacq);
    didcas = Cas(global_heapdescs,org,hd);
  } while (didcas == 0 && --iter);

  if (didcas == 0) hd->stat.nolink++; // not essential

  if (unlikely(id == 1)) init_env();
  hd->id = id;
  return hd;
}

static cchar *regname(xregion *reg)
{
  enum Rtype typ = reg->typ;
  return typ <= Rcount ? regnames[typ] : "??";
}

#include "dbg.h"

#include "diag.h" // main diags

// -- main heap init and access --

static _Atomic unsigned int global_mapadd;
static _Atomic unsigned int global_mapdel;

#if Yal_prep_TLS
 static bool yal_tls_inited; // set by accessing TLS from a 'constructor' aka .ini section function before main()
#endif

static ub1 mapshifts[24] = { // progressively increase region sizes the more we have
  0,0,0,0,
  0,0,1,1, // 8
  1,1,2,2,
  2,2,3,4, // 16
  5,6,7,8,
  10,12,14,14 };

#include "heap.h"

#define Logfile Fyalloc

static void *oom(ub4 fln,enum Loc loc,size_t n1,size_t n2)
{
  char buf[64];
  heapdesc *hd = thread_heap;

  if (n2) snprintf_mini(buf,0,64," * %zu`",n2);
  else *buf = 0;

  do_ylog(Yal_diag_oom,loc,fln,Error,0,"heap %u out of memory allocating %zu`%s",hd ? hd->id : 0,n1,buf);
  Enomem
  return nil;
}

static ub4 free2(ub4 fln,enum Loc loc,xregion *reg,size_t ip,size_t len,ub4 tag,cchar *msg)
{
  do_ylog(Yal_diag_dblfree,loc,fln,Error,1,"double free of ptr %zx len %zu` %s region %u.%u fretag %.01u %s ",ip,len,regnames[reg->typ],reg->hid,reg->id,tag,msg);
  return 0;
}

// Get chunk of memory from the O.S.
static void *osmem(ub4 fln,ub4 hid,size_t len,cchar *desc)
{
  void *p;

  if (len < 4096) do_ylog(Diagcode,Lnone,fln,Warn,0,"heap %u osmem len %u %s",hid,(ub4)len,desc);

  p = osmmap(len);
  ydbg3(Lnone,"osmem %zu` for %s at %u = %zx",len,desc,hid,(size_t)p)
  if (p) {
    Atomad(global_mapadd,1,Monone);
    return p;
  }

  errorctx(fln,Lnone,"heap %u %s",hid,desc)
  oom(Fln,Lnone,len,0);
  return p;
}

static bool osunmem(ub4 fln,heapdesc *hd,void *p,size_t len,cchar *desc)
{
  ydbg3(Lnone,"heap %u osunmem %zu` for %s at %u = %p",hd->id,len,desc,fln & Hi16,p)
  hd->stat.munmaps++;
  if (osmunmap(p,len)) {
    error2(Lnone,fln,"invalid munmap of %p for %s in heap %u - %m",p,desc,hd->id)
    return 1;
  }
  Atomad(global_mapdel,1,Monone);
  return 0;
}

#undef Logfile

static ub4 slabstats(region *reg,struct yal_stats *sp,char *buf,ub4 pos,ub4 len,bool print,ub4 opts,ub4 cnt);
#include "region.h"

#define Nolen (size_t)(-1)

#include "bump.h"
#include "mini.h"

#include "slab.h"

#include "free.h"
#include "alloc.h"
#include "realloc.h"

#include "stats.h"

#include "std.h"

#define Logfile Fyalloc

// -- nonstandard extensions
void *__je_bootstrap_malloc(size_t len)
{
  if (len >= Hi32) return nil;
  return bootalloc(Fln,0,Lnone,(ub4)len);
}

void *__je_bootstrap_calloc(size_t num, size_t size)
{
  if (size >= Hi32 || num >= Hi32) return nil;
  return __je_bootstrap_malloc(num * size); // bootmem is zeroed
}

void __je_bootstrap_free(void * Unused p) { } // no metadata or length

// --- optional ---

#if Yal_enable_extensions

void * yal_alloc(size_t size,unsigned int tag)
{
  return yalloc(size,size,Lalloc,tag);
}

void yal_free(void *p,unsigned int tag)
{
  yfree(p,0,tag);
}

void * yal_realloc(void *p,size_t newsize,unsigned int tag)
{
  return yrealloc(p,newsize,tag);
}

ub4 yal_options(enum Yal_options opt,size_t arg1,size_t arg2)
{
  ub4 rv;

  switch (opt) {
    case Yal_diag_enable: return diag_enable(arg1,(ub4)arg2);
    case Yal_trace_enable: return trace_enable((ub4)arg2);
    case Yal_logmask: rv = ylog_mask; ylog_mask = (ub4)arg1; return rv;
    default: do_ylog(Yal_diag_ill,Lnone,Fln,Warn,0,"unknown option '%d'",opt); return __LINE__;
  }
}

#endif // extensions

size_t malloc_usable_size(void * ptr)
{
  return yal_getsize(ptr,Fln);
}

#if Yal_mallopt

int mallopt(int param, int value)
{
  switch (param) {
  case M_MMAP_THRESHOLD:
  default: break;
  }
  return 0;
}
#endif

#if Yal_mallinfo

struct mallinfo2 *mallinfo2(void)
{
  static struct mallinfo2 mi;

  return &mi;
}

int malloc_info(int options, void *stream)
{
  return 0;
}
#endif

#if Yal_malloc_stats

void malloc_stats(void)
{
  yal_mstats(nil,1,0,"malloc_stats");
}
#endif

#if Yal_glibc_mtrace
void mtrace(void)
{
  ydbg1(Fln,Lnone,"region %zu` heap %zu`",sizeof(struct st_region),sizeof(struct st_heap))
  global_trace = 1;
}

void muntrace(void) {   global_trace = 0; }
#endif
